# Literature Review

# Background

Effective patient–provider communication is critical for quality care, yet measuring communication skills and interactions has long been difficult. Traditional assessment methods (e.g. observer ratings, patient surveys) are often subjective and inconsistent, requiring significant human effort (Berman & Chutka 2016). For instance, communication skills may be judged via patient feedback, self-assessment, or third-party observation – a multi-perspective approach that complicates standardization​ (Cubaka et. al. 2018). As a result, educators and researchers lack a rigorous, quantitative tool for evaluating communication efficacy over time. This gap has prompted interest in new solutions that can provide objective, scalable assessments of patient–provider conversations.

# LLMs in Health Care

Recent advances in large language models (LLMs) have opened avenues to enhance and evaluate healthcare communication. LLMs are AI systems trained on vast text data that can generate human-like responses​ . Both patients and clinicians are increasingly exploring AI-driven chatbots for medical information and advice (Huo et. al. 2025). Early findings show that generative AI chatbots can even respond to patients with appropriate expressions of empathy. Health systems have begun deploying LLMs to draft replies to patient messages in electronic health record portals, aiming to mitigate rising message volume and provider burnout (English et. al. 2024). This surge in interest has yielded a growing body of literature assessing LLM-based communication tools, with one recent review identifying 137 studies of chatbot-delivered health advice ​(Huo et. al. 2025). These studies have examined chatbots’ ability to summarize medical evidence, guide self-care, and answer patient questions on screening, diagnosis, and treatment. Overall, the emergence of LLMs in healthcare communication has set the stage for new approaches to assist providers and potentially improve patient engagement.

# LLM Response Generation

Collectively, current studies indicate that LLMs are adept at producing conversational content that is thorough and often empathetic. 

## Empathy and Tone: 

Multiple evaluations have now shown AI-generated messages scoring higher in empathy or compassion than typical human responses​ (Ayers et. al. 2023). LLMs like GPT-4 appear capable of recognizing emotional cues in a prompt and responding with supportive language, mimicking “bedside manner.” A recent review concluded that LLMs exhibit elements of cognitive empathy—they can acknowledge patient emotions and provide respectful, reassuring replies in various contexts​ (Sorin et. al. 2024). In some trials, these models even outperformed physicians in empathetic communication as perceived by patients or evaluators (Ayers et. al. 2023). However, there are nuances to this finding. Researchers have noted that AI empathy can sometimes ring hollow – for example, chatbots may overuse stock empathetic phrases or produce overly verbose sympathy that feels unnatural​ (Sorin et. al. 2024). Ensuring the authenticity and appropriateness of empathic responses remains an area for improvement. 

## Message Quality and Completeness

LLMs tend to generate replies that are longer and more detailed than those written under time pressure by clinicians​ (Ayers et. al. 2023). This verbosity can increase the informational content of a response, potentially addressing patient questions more fully. Indeed, studies have found AI responses often contain more comprehensive explanations or guidance, contributing to higher quality ratings​[pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/37115527/#:~:text=Chatbot%20responses%20were%20rated%20of,The%20proportion%20of). Yet excessive length or complexity may overwhelm some readers. Ayers et. al.’s finding that AI replies were less readable for patients with limited literacy highlights the importance of balancing completeness with clarity (Ayers et. al. 2023). It also suggests a need for tailoring AI outputs to match patient communication needs (e.g., simplifying language when needed).

# Automated Conversation Scoring \- Use Case

The use of LLMs to evaluate communication quality, not just generate it, represents a natural next step, addressing longstanding gaps in assessment. Historically, assessing a patient encounter or transcript required manual coding or ratings by trained observers – a time-consuming process prone to variability​ (Berman & Chutka 2016). Human evaluators might use checklists or rubrics to judge factors like clarity, empathy, and organization, but reaching consistent and quantifiable scores at scale is challenging. In contrast, an LLM-driven assessment tool could analyze transcripts rapidly and consistently against defined criteria. The literature reviewed above provides groundwork for such a tool: LLMs can detect nuances of tone, completeness of information, and even elements of empathy in text. In fact, researchers have begun formulating automatic metrics for empathy using LLMs (Luo et. al. 2024), and the positive correlations between LLM output characteristics and human satisfaction (ex., message length with satisfaction up to a point​ (Kim et. al. 2024\) offer insight into what makes communication effective. Moreover, experts are calling for more objective and standardized evaluation methods in this arena​ (Huo et. al. 2025). An automated conversation scoring system powered by LLMs can be designed to apply a consistent rubric (for example, scoring clarity, coherence, empathy, and accuracy on set scales), which directly responds to the critique that current evaluation of communication is too subjective ​(Huo et. al. 2025). By basing the LLM’s scoring criteria on evidence-based communication standards (such as using lay-friendly language, verifying understanding, and providing emotional support), the tool could produce ratings that align with what human experts would value, but with far greater speed and scalability. Such a system would fill a notable gap: providing quantifiable, reproducible metrics for communication quality that can be used to track improvement or compare interventions. Early efforts in this direction are promising. For example, a Mayo Clinic group demonstrated an automated “empathy score” by having an LLM rank physician vs AI responses, and found the model’s rankings correlated with human judgments of empathy (Luo et. al. 2024). While still preliminary, these advances suggest that LLMs could feasibly score real-world dialogue against communication best practices.

## Implications: Designing an LLM-Driven Communication Assessment Tool

The convergence of these findings informs the design of a novel LLM-based health communication assessment instrument. First, the tool can leverage LLMs’ demonstrated strength in analyzing language nuances – for instance, detecting whether a response addressed the patient’s question, used clear terminology, and conveyed empathy. The rubric-guided approach (scoring specific domains like clarity, conciseness, and emotional support) is supported by literature showing those domains are key to patient satisfaction and outcomes​ (Berman & Chutka 2016). Second, by automating the scoring, the tool overcomes the resource constraints of traditional assessments, enabling consistent evaluation at scale. This aligns with recent trends emphasizing real-time feedback and personalized learning in healthcare communication. Importantly, the tool’s outputs would need to be validated against human expert ratings to ensure credibility – a step very much in line with the calls for more rigorous, objective benchmarks in evaluating AI communication​ (Huo et. al. 2025). If successful, an LLM-driven scoring system could be used in educational settings (e.g., giving medical trainees immediate feedback on patient interview transcripts) or quality improvement (monitoring and enhancing how clinicians communicate via telehealth or patient portal messages). It essentially translates the qualitative aspects of conversation into quantitative scores without losing the context, something not previously achievable at scale. By filling this gap, the proposed method addresses the “lack of a rigorous and consistent mechanism” for communication assessment noted in Huo et. al. 2025\. It provides a new opportunity to systematically improve patient-centered communication: healthcare teams could identify specific weaknesses (like low clarity or empathy scores) and target them for training, and researchers could objectively measure whether an intervention (ex., a communication workshop or an AI coaching program) improves those scores over time. Of course, there are caveats. The tool must be carefully tested to avoid perpetuating biases (for example, not penalizing non-native speakers unfairly on language use) and to ensure it complements rather than replaces human judgment. Any score for a critical skill like communication should be interpreted in context. Nonetheless, the literature suggests that with proper design and oversight, LLM-driven analysis can substantially augment how we assess and ultimately enhance patient–provider conversations. This represents a significant step toward more empathetic, clear, and effective communication in healthcare, guided by data-driven insights and the power of modern AI.

# LLM Optimism in Scoring

In deploying large language models (LLMs) for evaluating patient communication, it is important to consider the model's tendency toward over-optimism during decoding, particularly when using beam search. This over-optimism arises from inflated internal Q-value estimations, leading the model to over-score suboptimal or insufficiently supported responses (Zhang et. al. 2025). Such behavior can result in disproportionately high ratings for vague, verbose, or off-topic statements, thereby compromising the integrity of automated scoring systems. Moreover, low-temperature decoding, commonly used to promote determinism and low variability, can exacerbate this issue by further sharpening the model’s confidence distribution, amplifying the likelihood of committing to erroneous but seemingly high-confidence outputs. To address this, we highlight the relevance of recent developments such as Supervised Optimism Correction (SOC), which introduces an auxiliary loss during fine-tuning to calibrate model confidence toward expert-demonstrated responses. When paired with calibrated temperature scaling during inference, these strategies collectively help mitigate inflated scoring and promote more reliable, interpretable, and fair assessment outcomes. This is particularly critical in high-stakes domains like healthcare, where unjustified confidence in communication quality could obscure areas needing targeted intervention.

# Overall Notes:

Clearly, there exists research regarding the assessment and analysis of physician communication. There’s a clear emphasis on conveyed empathy, clarity, etc. Additionally, there exist explorations into the value of LLMs in improving physician communication with their patients, even showing evidence that LLM-driven communication may be more comprehensive and perceived as more empathetic. This establishes the foundation for justifying the efficacy of LLM-generated synthetic conversations, as it’s likely to be more empathetic and comprehensive than even the average human patient-physician interaction. Thus, recent literature seems to validate our method of synthetic conversation generation and consequent rubric optimization.

However, there remains a distinct lack of literature on assessing patient communication ability. This paper serves as a novel dive into the assessment of patient communication ability using a rubric-based LLM assessment tool.

# Patient-Doctor Transcripts

Department of Veterans Affairs Transcripts

- 405 real patient-doctor transcripts

Nature Paper Simulated Interactions Transcripts

- 272 simulated patient-doctor transcripts

